{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тетрадка № 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь будет обсуждаться новая модель синхронизации: смесь синхронизации по \"естественной\" топологии $A$ и по полносвязной топологии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обозначения : \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& A (n \\times n): a_{ij} = \\dfrac{1}{d(i)} I(j \\in N(i)) - \\text{матрица инцидентности}\\\\\n",
    "& d(i) = \\sum\\limits_{j} I(a_{ij} > 0) = |N(i)| - \\text{степени вершин}\\\\\n",
    "& \\pi(A) = \\pi = \\frac{1}{\\sum d(i)} (d(1), \\ldots, d(n))^T, \\quad \\pi^T A = \\pi^T \\\\\n",
    "& \\tau = \\max\\limits_{x \\perp \\pi} \\dfrac{||Ax||_2}{||x||_2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее рассматривалась синхронизация по такому принципу:\n",
    "$$\n",
    "x(t + 1) = Ax(t) + w(t),\n",
    "$$\n",
    "где $w(t)$ - шум с нулевым средним и диагональной матрицей ковариации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг 1. Переход к классу матриц\n",
    "В изученной ранее модели в любой момент времени обмен сигналами происходил согласно топологии, диктуемой матрицей $A$. Рассмотрим задачу, когда в момент времени $t$ синхронизация происходит с помощью матрицы $A(t)$:\n",
    "$$\n",
    "x(t + 1) = A(t)x(t) + w(t)\n",
    "$$\n",
    "где матрицы $A(t)$ берутся из некоторого класса:\n",
    "$$\n",
    "\\forall t \\quad A(t) \\in M_{\\pi} = \\left\\{A| \\pi(A) = \\pi\\right\\}\n",
    "$$\n",
    "То есть все рассматриваемые матрицы объединены свойством: у них общий левый собственный вектор $\\pi$. С точки зрения топологии коммуникации это значит, что агенты  с течением времени не меняют своего числа соседей. На практике это может быть связано с тем, что дешевле перераспределить сигналы, чем менять способность передатчиков принимать то или иное число сигналов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг 2. Введение \"супервизии\"\n",
    "Рассмотрим такое преобразование над матрицей $A$:\n",
    "$$\n",
    "B_{\\varkappa} = \\varkappa A + (1 - \\varkappa) \\mathbb{1} \\pi^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у матрицы $A$ были собственные вектора:\n",
    "$$\n",
    "\\pi^T A = \\pi^T \\\\\n",
    "e_j^T A = \\lambda_j e_j^T \\quad (\\lambda_j \\neq 1)\n",
    "$$\n",
    "Посмотрим, какие собственные вектора у матрицы $B_{\\varkappa}$:\n",
    "$$\n",
    "\\pi^T B_{\\varkappa} = \\varkappa \\pi^T + (1 - \\varkappa) \\pi^T \\mathbb{1} \\pi^T = \\pi^T  \\\\\n",
    "e_j^T B_{\\varkappa} = \\varkappa \\lambda_j e_j^T + (1 - \\varkappa) e_j^T \\mathbb{1} \\pi^T\n",
    "$$\n",
    "Покажем, что $e_j^T \\mathbb{1} = 0$:\n",
    "$$\n",
    "e_j^T \\mathbb{1} = e_j^T A \\mathbb{1} = \\lambda_j e_j^T \\mathbb{1} \\Rightarrow (1 - \\lambda_j)e_j^T \\mathbb{1} = 0\n",
    "$$\n",
    "Поскольку $\\lambda_j \\neq 1$, то $e_j^T \\mathbb{1} = 0$. Отсюда следует, что\n",
    "$$\n",
    "e_j^T B_{\\varkappa} = \\varkappa \\lambda_j e_j^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что произошло с величиной $\\tau^2$ для матрицы $B_{\\varkappa}$. Для этого возьмём вектор $y$:\n",
    "$$\n",
    "y^T \\pi = 0 \\\\\n",
    "y^T y = 1\n",
    "$$\n",
    "Запишем:\n",
    "$$\n",
    "y^T B_{\\varkappa}^T B_{\\varkappa} y = \\varkappa^2 y^T A^T A y + \\varkappa (1 - \\varkappa) y^T A^T\\mathbb{1} \\pi^T y + \n",
    "\\varkappa (1 - \\varkappa) y^T \\pi \\mathbb{1}^T A y + (1-\\varkappa)^2 y^T \\pi \\mathbb{1}^T \\mathbb{1} \\pi^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что в последних трёх слагаемых присутствуют множители $y^T \\pi$ или $\\pi^T y$, которые равны нулю. Поэтому\n",
    "$$\n",
    "y^T B_{\\varkappa}^T B_{\\varkappa} y = \\varkappa^2 y^T A^T A y \\\\\n",
    "\\tau_{\\varkappa} = \\max\\limits_{x \\perp \\pi} \\dfrac{||B_{\\varkappa}x||_2}{||x||_2} =  \\varkappa \\max\\limits_{x \\perp \\pi} \\dfrac{||Ax||_2}{||x||_2} = \\varkappa \\tau\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Итог:\n",
    "Если мы будем брать матрицы $A(t)$ из класса $M_{\\pi}$, применять к ним преобразование с параметром $\\varkappa$ и производить синхронизацию по обновлённой топологии $B_{\\varkappa}(t)$, то мы всегда сможем подобрать параметр $\\varkappa$ так, чтобы получилась оценка на предельную дисперсию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P.S.: Что означает синхронизация через $\\mathbb{1} \\pi^T$?\n",
    "Рассмотрим матрицу $F = \\mathbb{1} \\pi^T$:\n",
    "$$\n",
    "F = \\frac{1}{\\sum d(i)} \n",
    "\\begin{pmatrix}\n",
    "& d(1) &d(2) &\\ldots &d(n) \\\\\n",
    "& \\vdots & \\ddots& \\ddots&\\vdots \\\\\n",
    "& d(1) &d(2) &\\ldots &d(n) \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "x(t+1) = F x(t) \\Leftrightarrow x_i(t + 1) = \\frac{\\sum_{j} d(j) x_j(t)}{\\sum_{j} d(j)} \\\\\n",
    "x(t+1) = A x(t) \\Leftrightarrow x_i(t + 1) = \\frac{\\sum_{j \\in N(i)}  x_j(t)}{d(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня была такая мысль: рассмотрим валентность вершины как некоторый потенциал. В случае синхронизации через $F$ каждая вершина получает сигнал от всех остальных пропорционально их потенциалам.\n",
    "В случае же синхронизации через $A$ каждая вершина получает сигналы от соседей, равномерно их усредняя (то есть деля на свой потенциал). \n",
    "\n",
    "И вот здесь получается какая-то несимметричность: в одном случае потенциал диктует силу передачи, а в другом силу приёма. Быть, может, в будущем получится рассмотреть другую схему синхронизации по естественной топологии: для этого нужен ориентированный граф."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Почему мне кажется хорошей идея рассмотрения ориентированных графов:\n",
    "* для них естественней рассматривать сохранение валентностей: каждая вершина $i$ *независимо* выбирает $d(i)$ клиентов, которым она передаст сигнал\n",
    "* естественней выглядит смешение с матрицей $F$: без неё вершина $i$ равномерно распределяет потенциал между своими топологическими соседями. А по матрица $(1 - \\varkappa)F$ заставляет её взять часть потенциала и равномерно распределить между всеми агентами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть каждый агент получает сигнал от всех остальных пропорционально их валентностям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос на будущее: какие классы матриц $A(t)$ рассматривать, чтобы\n",
    "* их было легко перестраивать, сохраняя валентности\n",
    "* можно было написать какую-нибудь оценку на $\\tau$ и из неё получить оценку на $\\varkappa$\n",
    "* топология смотрелась как-нибудь естественно (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
